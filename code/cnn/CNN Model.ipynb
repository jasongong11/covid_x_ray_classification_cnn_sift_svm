{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08cb185",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code only implements CNN model with batch size 64 and get the results of confusion matrix based on test set\n",
    "and also implements the imagine processing before CNN model. As for other CNN models listed in the report, you \n",
    "only need to change the parameter in the layer. For adding intercepts, use bias_initializer=\"ones\"; for regularization,\n",
    "use kernel_regularizer=regularizers.l2(0.01)(for L2 penalty) and kernel_regularizer=regularizers.l1(0.01)(for L1 penalty);\n",
    "for average global pooling, use cnn.add(GlobalAveragePooling2D()) instead of cnn.add(Flatten())\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPooling2D,GlobalAveragePooling2D\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "#Get the train data and test data\n",
    "train=pd.read_csv('../train_update.csv', sep=\",\")\n",
    "test=pd.read_csv('../test.txt', sep=\" \",header=None)\n",
    "train.drop('Unnamed: 0', axis=1,inplace=True)\n",
    "test.columns=[\"patient_id\",\"filename\",\"labels\",\"sourses\"]\n",
    "\n",
    "\n",
    "dstpath='..'\n",
    "path_train=[]\n",
    "for line in train[\"filename\"]:\n",
    "    path_train.append(dstpath+'/'+line)\n",
    "path_train=pd.DataFrame({'filename':path_train})\n",
    "labels=train[\"classes\"].copy()\n",
    "train_df=pd.concat([path_train,labels],axis=1)\n",
    "train_df.columns=[\"filename\",\"labels\"]\n",
    "\n",
    "#set the features of imagine\n",
    "img_height = 300\n",
    "img_width = 300\n",
    "batch_size=64\n",
    "img_size=(img_height, img_width)\n",
    "\n",
    "# Create Image Data Generator for Train Set\n",
    "image_gen = ImageDataGenerator(\n",
    "                                  rescale = 1./255,\n",
    "                                  shear_range = 0.2,\n",
    "                                  zoom_range = 0.2,\n",
    "                                  horizontal_flip = True,          \n",
    "                               )\n",
    "\n",
    "# Create Image Data Generator for Test/Validation Set\n",
    "test_data_gen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "def scalar(img):\n",
    "    return img/127.5-1  # scale pixel between -1 and +1\n",
    "train=image_gen.flow_from_dataframe(train_df, x_col='filename', y_col='labels', \n",
    "                                    target_size=img_size, color_mode='grayscale'\n",
    "                                    ,class_mode='binary',batch_size=batch_size)\n",
    "\n",
    "\n",
    "test=test_data_gen.flow_from_dataframe(test,'../', \n",
    "                                       x_col='filename', y_col='labels', target_size=img_size, \n",
    "                                       color_mode='grayscale',\n",
    "                                       shuffle=False, \n",
    "#setting shuffle as False just so we can later compare it with predicted values without having indexing problem \n",
    "                                       class_mode='binary',batch_size=batch_size)\n",
    "\n",
    "#plot the x-ray figures\n",
    "plt.figure(figsize=(12, 12))\n",
    "for i in range(0, 10):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    for X_batch, Y_batch in train:\n",
    "        image = X_batch[0]        \n",
    "        dic = {0:'negative', 1:'positive'}\n",
    "        plt.title(dic.get(Y_batch[0]))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(np.squeeze(image),cmap='gray',interpolation='nearest')\n",
    "        break\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Build the CNN model\n",
    "cnn = Sequential()\n",
    "cnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "cnn.add(Flatten())\n",
    "cnn.add(Dense(activation = 'relu', units = 128,kernel_regularizer=regularizers.l2(0.01)))\n",
    "cnn.add(Dense(activation = 'relu', units = 64,kernel_regularizer=regularizers.l2(0.01)))\n",
    "cnn.add(Dense(activation = 'sigmoid', units = 1,kernel_regularizer=regularizers.l2(0.01)))\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "cnn.summary()\n",
    "\n",
    "#Set the stopping rules in each epoch\n",
    "early = EarlyStopping(mode=\"min\", patience=3)\n",
    "learning_rate_reduction = ReduceLROnPlateau(patience = 2, verbose=1,factor=0.3, min_lr=0.000001)\n",
    "callbacks_list = [ early, learning_rate_reduction]\n",
    "\n",
    "#Set the weight to each class to keep balance of samples\n",
    "weights = compute_class_weight('balanced', np.unique(train.classes), train.classes)\n",
    "cw = dict(zip( np.unique(train.classes), weights))\n",
    "print(cw)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Fit the cnn model\n",
    "history=cnn.fit(train,epochs=10, class_weight=cw, callbacks=callbacks_list)\n",
    "\n",
    "#Put the test set into the model\n",
    "test_accu = cnn.evaluate(test)\n",
    "print('The testing accuracy is :',test_accu[1]*100, '%')\n",
    "preds = cnn.predict(test,verbose=1)\n",
    "predictions = preds.copy()\n",
    "predictions[predictions <= 0.5] = 0\n",
    "predictions[predictions > 0.5] = 1\n",
    "\n",
    "#Get the confusion matrix\n",
    "cm = pd.DataFrame(data=confusion_matrix(test.classes, predictions, labels=[0, 1]),index=[\"Actual Negative\", \"Actual Positive\"],\n",
    "columns=[\"Predicted Negative\", \"Predicted Positive\"])\n",
    "\n",
    "#Draw the confusion matrix and save the figure\n",
    "sns_plot=sns.heatmap(cm,annot=True,fmt=\"d\")\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig('../cnn_method.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
